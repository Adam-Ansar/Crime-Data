{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Gun Crime analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "1. **Load the Dataset**: Import the necessary CSV files into dataframes.\n",
        "2. **Explore the Data**: Perform initial exploration to understand the structure and content of the data.\n",
        "3. **Data Cleaning and Refinement**: Clean the data by handling missing values, correcting data types, and refining the dataset for analysis.\n",
        "4. **Surface Level Analysis**: Conduct preliminary analysis to identify trends, patterns, and key statistics.\n",
        "5. **Basic Visualizations**: Create visual representations of the data to aid in understanding and communicating findings.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* \n",
        "## Outputs\n",
        "\n",
        "* \n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Check if the file exists in the current directory\n",
        "file_path = os.path.join(current_dir, 'gun_deaths.csv')\n",
        "if os.path.exists(file_path):\n",
        "    print(\"File exists\")\n",
        "else:\n",
        "    print(\"File does not exist\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Section 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Exploration, Cleaning, and Refinement\n",
        "\n",
        "### Data Exploration\n",
        "Data exploration involves examining the dataset to understand its structure, content, and key characteristics. This step includes:\n",
        "1. **Loading the Dataset**: Importing the dataset into a DataFrame.\n",
        "2. **Initial Exploration**: Displaying the first few rows to get a sense of the data.\n",
        "3. **Summary Statistics**: Using methods like `info()` and `describe()` to get an overview of the dataset, including data types, missing values, and basic statistical measures.\n",
        "4. **Identifying Missing Values**: Checking for missing values in the dataset, which can affect analysis and need to be addressed.\n",
        "5. **Identifying Duplicate Rows**: Checking for duplicate rows that can inflate the dataset and skew analysis results.\n",
        "\n",
        "### Data Cleaning\n",
        "Data cleaning involves handling issues identified during exploration to ensure the dataset is accurate and reliable. This step includes:\n",
        "1. **Handling Missing Values**: Addressing missing values by either filling them with appropriate values (e.g., mean, median) or removing rows/columns with excessive missing data.\n",
        "2. **Removing Duplicate Rows**: Dropping duplicate rows to ensure each entry in the dataset is unique.\n",
        "3. **Correcting Data Types**: Ensuring that each column has the correct data type (e.g., converting columns to categorical or numerical types as needed).\n",
        "\n",
        "### Data Refinement\n",
        "Data refinement involves further processing the cleaned dataset to prepare it for analysis. This step includes:\n",
        "1. **Feature Engineering**: Creating new features or modifying existing ones to enhance the dataset's predictive power.\n",
        "2. **Normalization and Scaling**: Normalizing or scaling numerical features to ensure they are on a similar scale, which can improve the performance of certain algorithms.\n",
        "3. **Encoding Categorical Variables**: Converting categorical variables into numerical format using techniques like one-hot encoding or label encoding.\n",
        "\n",
        "By following these steps, we ensure that the dataset is well-prepared for meaningful analysis and visualization, leading to more accurate and insightful results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Importing the required libraries\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.stats as stats\n",
        "import statsmodels.api as sm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importing the required libraries is essential and the foundation of any data analysis of a dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('gun_deaths.csv')\n",
        "\n",
        "# Display the first 5 rows of the dataset\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```markdown\n",
        "# Initial Exploration of the Dataset\n",
        "\n",
        "The first five rows of the dataset provide a snapshot of the data structure and content. Here are some key observations:\n",
        "\n",
        "1. **Columns**:\n",
        "    - `year`: The year in which the incident occurred.\n",
        "    - `month`: The month in which the incident occurred.\n",
        "    - `intent`: The intent behind the incident (e.g., Suicide, Homicide).\n",
        "    - `police`: Indicates whether a police officer was involved (0 for no, 1 for yes).\n",
        "    - `sex`: The gender of the victim (M for male, F for female).\n",
        "    - `age`: The age of the victim.\n",
        "    - `race`: The race of the victim.\n",
        "    - `place`: The location where the incident occurred.\n",
        "    - `education`: The education level of the victim.\n",
        "\n",
        "2. **Data Types**:\n",
        "    - The `year`, `month`, and `police` columns are of integer type.\n",
        "    - The `age` column is of float type, indicating that it may contain missing values.\n",
        "    - The `intent`, `sex`, `race`, `place`, and `education` columns are of object type, representing categorical data.\n",
        "\n",
        "3. **Missing Values**:\n",
        "    - The `intent` column has 1 missing value.\n",
        "    - The `age` column has 18 missing values.\n",
        "    - The `place` column has 1,384 missing values.\n",
        "    - The `education` column has 1,422 missing values.\n",
        "\n",
        "4. **Duplicate Rows**:\n",
        "    - There are 39,227 duplicate rows in the dataset, which need to be addressed during data cleaning.\n",
        "\n",
        "5. **Distribution of Education Levels**:\n",
        "    - The dataset contains various education levels, with `HS/GED` being the most common, followed by `Less than HS`, `Some college`, and `BA+`. There are also 1,422 missing values in the `education` column.\n",
        "\n",
        "These initial findings highlight the need for data cleaning and refinement to handle missing values, remove duplicate rows, and ensure accurate data types for meaningful analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get a summary of the dataset\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```markdown\n",
        "The `data.info()` method provides a concise summary of the DataFrame, which includes the number of entries (rows), the number of columns, and the data types of each column. It also shows the non-null count for each column, indicating how many non-missing values are present. This summary helps in understanding the structure of the dataset, identifying columns with missing values, and verifying the data types of each column. For instance, in our dataset, we have 100,798 entries and 9 columns, with some columns containing missing values (e.g., 'intent', 'age', 'place', and 'education'). The data types include integers, floats, and objects (categorical data).\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get descriptive statistics for the dataset because the dataset contains numerical and categorical variables\n",
        "data.describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```markdown\n",
        "The `df.describe()` method in pandas provides a summary of the central tendency, dispersion, and shape of a dataset's distribution, excluding `NaN` values. It generates descriptive statistics for numerical columns by default, including:\n",
        "\n",
        "- **Count**: The number of non-null entries.\n",
        "- **Mean**: The average value.\n",
        "- **Std**: The standard deviation, which measures the spread of the data.\n",
        "- **Min**: The minimum value.\n",
        "- **25%**: The 25th percentile (first quartile).\n",
        "- **50%**: The 50th percentile (median or second quartile).\n",
        "- **75%**: The 75th percentile (third quartile).\n",
        "- **Max**: The maximum value.\n",
        "\n",
        "For example, calling `data.describe()` on our dataset will provide these statistics for columns like `year`, `month`, `age`, etc., helping us understand the distribution and variability of the data.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values as it may affect the analysis\n",
        "missing_values = data.isnull().sum()\n",
        "missing_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```markdown\n",
        "The `data.isnull()` method in pandas is used to detect missing values in the DataFrame. It returns a DataFrame of the same shape as `data`, with boolean values indicating whether an element is missing (`True`) or not (`False`). By summing the result with `data.isnull().sum()`, we get the count of missing values for each column.\n",
        "\n",
        "### Findings:\n",
        "- `intent`: 1 missing value\n",
        "- `age`: 18 missing values\n",
        "- `place`: 1,384 missing values\n",
        "- `education`: 1,422 missing values\n",
        "\n",
        "These missing values need to be addressed during data cleaning to ensure accurate analysis.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for duplicate rows in the dataset\n",
        "duplicate_rows = data.duplicated().sum()\n",
        "duplicate_rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explanation of Duplicate Rows Result\n",
        "\n",
        "The variable `duplicate_rows` holds the count of duplicate rows in the dataset. Duplicate rows can inflate the dataset and affect the accuracy of the analysis. Identifying and removing duplicate rows is an essential step in data cleaning to ensure the dataset's integrity.\n",
        "\n",
        "1. **duplicate_rows**:\n",
        "    - **Type**: `numpy.int64`\n",
        "    - **Value**: `39227`\n",
        "    - **Description**: This variable indicates that there are 39,227 duplicate rows in the dataset. These duplicates need to be addressed during the data cleaning process to ensure accurate and reliable analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Begin Data Cleaning and Refinement\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop duplicate rows in the dataset\n",
        "data = data.drop_duplicates()\n",
        "\n",
        "# Check for duplicate rows in the dataset\n",
        "duplicate_rows = data.duplicated().sum()\n",
        "duplicate_rows\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now acting upon the findings found during the data exploration, the duplicated rows have been dropped. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handling missing values\n",
        "data['intent'].fillna(data['intent'].mode()[0], inplace=True)\n",
        "data['age'].fillna(data['age'].median(), inplace=True)\n",
        "data['place'].fillna('Unknown', inplace=True)\n",
        "data['education'].fillna('Unknown', inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correcting data types\n",
        "data['year'] = data['year'].astype('category')\n",
        "data['month'] = data['month'].astype('category')\n",
        "data['police'] = data['police'].astype('category')\n",
        "data['sex'] = data['sex'].astype('category')\n",
        "data['race'] = data['race'].astype('category')\n",
        "data['place'] = data['place'].astype('category')\n",
        "data['education'] = data['education'].astype('category')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify the changes\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Section 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Section 2 content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* You may add as many sections as you want, as long as it supports your project workflow.\n",
        "* All notebook's cells should be run top-down (you can't create a dynamic wherein a given point you need to go back to a previous cell to execute some task, like go back to a previous cell and refresh a variable content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* In cases where you don't need to push files to Repo, you may replace this section with \"Conclusions and Next Steps\" and state your conclusions and next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKlnIozA4eQO",
        "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "  # Create a new folder in the current directory\n",
        "  os.makedirs(os.path.join(current_dir, 'new_folder'))\n",
        "except Exception as e:\n",
        "  print(e)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
